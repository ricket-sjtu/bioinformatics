{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02: Introduction to Next-generation sequencing (NGS)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Next generation sequencing (a.k.a high-throughput sequencing), is a term used to describe a set of the state-of-the-art sequencing technologies:\n",
    "- **Illumina** (Solexa) Sequencing\n",
    "- **Roche 454** Sequencing\n",
    "- **ABI SOLiD** Sequencing\n",
    "- **Ion Torrent**: Proton/PGM Sequencing\n",
    "\n",
    "These recent technologies apply a more efficient and cheaper approaches than the traditional Sanger sequencing technology, which is the most popular sequencing techniques in the past 20 years. And thus this have revolutionalized the study of genomics and molecular biology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of NGS\n",
    "\n",
    "The four main advantages of these high-throughput sequencing technologies over Sanger sequencing are:\n",
    "\n",
    "- **Shorter read length and more repeats**\n",
    "\n",
    "Sanger sequencing can be used to give much longer sequence reads. However, the parallel nature of NGS means that longer reads can be constructed from many contiguous short reads.\n",
    "\n",
    "Repeats are intrinsic to NGS, as each read is amplified before sequencing, and because it relies on many short overlapping reads, so each section of DNA or RNA is sequenced multiple times. Also, because it is so much quicker and cheaper, it is possible to do more repeats than with Sanger sequencing.\n",
    "\n",
    "- **Much higher efficiency**\n",
    "\n",
    "NGS is much more efficent than Sanger in two ways. \n",
    "\n",
    "Firstly, the chemical reaction may be combined with the signal detection in some versions of NGS, whereas in Sanger sequencing these are two separate processes. \n",
    "\n",
    "Secondly and more significantly, only one read (maximum ~1kb) can be taken at a time in Sanger, whereas NGS does essentially adopt the **massively parallel techniques**, allowing 300Gb of DNA to be read on a single run on a single chip. And that's why it is called **high-throughput**.\n",
    "\n",
    "- **Lower cost**\n",
    "\n",
    "The reduced time, manpower and reagents in NGS mean that the costs are much lower. The first human genome sequence cost in the region of £300M. Using modern Sanger sequencing methods, aided by data from the known sequence, a full human genome would still cost £6M. Sequencing a human genome with Illumina today would cost only £6,000.\n",
    "\n",
    "\n",
    "- **Higher accuracy**\n",
    "\n",
    "On the level of individual base, the NGS is of lower accuracy. However, due to the existence of a large number of repeats and hence the resulting much higher depth and coverage, the NGS can achieve a higher overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Tasks in NGS Sequence Analysis\n",
    "\n",
    "- Sequence formats and quality assessment (序列格式与序列质量评估).\n",
    "- Mapping of NGS to reference sequences (映射到参考序列).\n",
    "- Identification and labelling of variants, such as SNPs and structural variations (发现序列中变异如SNVs和SVs).\n",
    "- Sequence assembly (序列拼接)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Sequencing Data\n",
    "\n",
    "The raw sequencing data are stored as **FASTQ** format, which is derived from the Sanger standard for capillary data. The format is somewhat simiar to the FASTA format, but it also hosts both the sequences and the associated **per-base quality score**, which PHRED quality scores encoded as ASCII printable characters (33-126).\n",
    "\n",
    "Here is an example:\n",
    "```\n",
    "@SEQ_ID\n",
    "GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n",
    "+\n",
    "!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n",
    "```\n",
    "\n",
    "#### Illumina sequence identifiers\n",
    "\n",
    "Sequences from the Illumina software use a systematic identifier:\n",
    "```\n",
    "@EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG\n",
    "```\n",
    "which contains the following information:\n",
    "\n",
    "| **Term** | **Description** |\n",
    "| --- | --- |\n",
    "| **EAS139** |\tthe unique instrument name |\n",
    "| **136**\t| the run id |\n",
    "| **FC706VJ** |\tthe flowcell id |\n",
    "| **2** | flowcell lane |\n",
    "| **2104** | tile number within the flowcell lane |\n",
    "| **15343** | 'x'-coordinate of the cluster within the tile |\n",
    "| **197393** |\t'y'-coordinate of the cluster within the tile |\n",
    "| **1** | the member of a pair, 1 or 2 (paired-end or mate-pair reads only) |\n",
    "| **Y**\t | Y if the read is filtered, N otherwise |\n",
    "| **18** | 0 when none of the control bits are on, otherwise it is an even number |\n",
    "| **ATCACG** | index sequence |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHRED score system\n",
    "\n",
    "A Phred score of a base is\n",
    "$$\n",
    "Q_{phred} = -10 \\log_{10} e\n",
    "$$\n",
    "where $e$ is the estimated probability of a base being an sequencing error.\n",
    "\n",
    "After Illumina 1.8, the quality scores for Illumina reads have basically adopted the Sanger format (Phred+33), which encode a Phred quality score from 0 to 93 using ASCII 33 to 126 (although in raw read data the Phred quality score rarely exceeds 60, higher scores are possible in assemblies or read maps).\n",
    "\n",
    "Here is the score system for different platforms:\n",
    "```\n",
    "  SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.....................................................\n",
    "  ..........................XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX......................\n",
    "  ...............................IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII......................\n",
    "  .................................JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ......................\n",
    "  LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL....................................................\n",
    "  !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
    "  |                                  |    |      |                                                                                         |                                                                                                              |\n",
    " 33                            59 64   73                                                                                  104                                                                                                        126                           \n",
    "\n",
    " S - Sanger        Phred+33,  raw reads typically (0, 40)\n",
    " X - Solexa        Solexa+64, raw reads typically (-5, 40)\n",
    " I - Illumina 1.3+ Phred+64,  raw reads typically (0, 40)\n",
    " J - Illumina 1.5+ Phred+64,  raw reads typically (3, 40)\n",
    "     with 0=unused, 1=unused, 2=Read Segment Quality Control Indicator (bold) \n",
    "     (Note: See discussion above).\n",
    " L - Illumina 1.8+ Phred+33,  raw reads typically (0, 41)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 1</font>\n",
    "\n",
    "1. Write a Python script to convert the **PHRED+33 ASCII-encoded quality string** to the corresponding estimated probability of the base being a sequencing error.\n",
    "\n",
    "2. Given a FASTQ-formatted read below, can you compute the probability that the sequenced read is completely correct? Is this a good or bad read?\n",
    "```\n",
    "@IL4_315:7:105:408:43\n",
    "ATTTGGCTCTCTGCTTGTTTATTATTGGTGTATNGG\n",
    "+\n",
    "+1,1+16;>;166>;>;;>>;>>>>>>,>>>>>+>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.981071705534969e-07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10** (-(ord('a') - 33)/10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mapping: Align the reads to the reference genome\n",
    "\n",
    "One of the fundamental ideas for mapping the reads to the reference genome is the Burrow's Wheeler Transform (BWT) algorithm, which is also one of the popular algorithms for text compression used by `bzip2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Burrows-Wheeler transform (BWT)\n",
    "\n",
    "For a input string, how can we build the sorted Burrows-Wheeler array (BWA) and conduct the Burrows-Wheeler transform?\n",
    "\n",
    "It's very simple by first writing down all the possible rotations of the string, and then sort them lexically. \n",
    "\n",
    "![](../images/bwt1.gif)\n",
    "\n",
    "Therefore, $BWA(T)$ is the lexical sorting of `T`, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ACAACG\n",
      "AACG$AC\n",
      "ACAACG$\n",
      "ACG$ACA\n",
      "CAACG$A\n",
      "CG$ACAA\n",
      "G$ACAAC\n"
     ]
    }
   ],
   "source": [
    "def bwa(t):\n",
    "    t = t + \"$\"\n",
    "    bwa = []\n",
    "    lt = len(t)\n",
    "    for i in range(lt):\n",
    "        bwa.append(t[i:] + t[:i])\n",
    "        \n",
    "    bwa.sort()\n",
    "    return bwa\n",
    "\n",
    "for s in bwa(\"ACAACG\"):\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then write down the last column as the `BWT(T)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC$AAAC\n"
     ]
    }
   ],
   "source": [
    "def bwt(t):\n",
    "    bw = bwa(t)\n",
    "    lt = len(t)\n",
    "    bwt = [seq[lt] for seq in bw]\n",
    "    \n",
    "    return ''.join(bwt)\n",
    "\n",
    "print bwt(\"ACAACG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reconstruct the sequence from the BWT string\n",
    "\n",
    "![](../images/bwt2.gif)\n",
    "![](../images/bwt3.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "ACAACG\n"
     ]
    }
   ],
   "source": [
    "bwt = 'GC$AAAC'\n",
    "\n",
    "def occ(ch):\n",
    "    return sum([c<ch for c in bwt])\n",
    "\n",
    "def count(ch, idx):\n",
    "    return sum([c==ch for c in bwt[:idx]])\n",
    "\n",
    "def LF(ch,  idx):\n",
    "    return occ(ch) + count(ch, idx)\n",
    "\n",
    "def reconstruct():\n",
    "    i = 0\n",
    "    t = ''\n",
    "    while not bwt[i] == '$':\n",
    "        t = bwt[i] + t\n",
    "        print i\n",
    "        i = LF(bwt[i], i)\n",
    "        \n",
    "    return t\n",
    "\n",
    "seq = reconstruct()\n",
    "print seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/bwt4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full-text Minute-size index (FM Index)\n",
    "\n",
    "How do we convert a genome into an alternate representation that permits rapid matching of millions of sequence reads? FM-index and BWT, in my opinion, stands for the same thing, is used for creating index for a long text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read Alignment\n",
    "\n",
    "How can we use an FM index and BWT to rapidly align reads to a refernce genome?\n",
    "\n",
    "### 3.1 Exact matching with FM Index\n",
    "\n",
    "For a query sequence, we can use an iterative method to align:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "4 6\n",
      "2 4\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "genome = \"ACAACG\"\n",
    "bwt = bwt( genome)\n",
    "query = \"AAC\"\n",
    "top = 0\n",
    "bot = len(bwt)\n",
    "print top, bot\n",
    "for ch in query[::-1]:\n",
    "    top = LF(ch, top)\n",
    "    bot = LF(ch, bot)\n",
    "    print top, bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ACAACG\n",
      "AACG$AC\n",
      "ACAACG$\n",
      "ACG$ACA\n",
      "CAACG$A\n",
      "CG$ACAA\n",
      "G$ACAAC\n"
     ]
    }
   ],
   "source": [
    "def printit(t):\n",
    "    for i in t:\n",
    "        print i\n",
    "\n",
    "printit( bwa(genome)[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAACG$A\n",
      "CG$ACAA\n"
     ]
    }
   ],
   "source": [
    "printit(bwa(genome)[4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACAACG$\n",
      "ACG$ACA\n"
     ]
    }
   ],
   "source": [
    "printit(bwa(genome)[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACG$AC\n"
     ]
    }
   ],
   "source": [
    "printit(bwa(genome)[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Explanation of the algorithm</font>\n",
    "\n",
    "In each iteration, **top** and **bot** delimit the range of rows beginning with progressively longer suffixes of the `query` (i.e., `top` and `bot`defines the ranges of rows of Burrows-Wheeler Array (BWA) that exactly match the current suffix of the `query` reads), and suffix of the `query` extends a base longer in each iteration.  So if range becomes empty (that is, `top = bot`), the `query suffix` (and therefore the `query` itself) does not occur in the `genome`.\n",
    "\n",
    "So we can see that the time complexity of the matching is $\\mathcal{O}(m)$ where $m$ is the length of the reads. That is, the algorithm is linear complexity, right?\n",
    "\n",
    "So once we've built the index of the genome, we can use it again and again. We don't need to rebuild the index, right? You don't have to pay for the time to build this index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find the exact hits in the genome\n",
    "\n",
    "Now we know that our query has a hit in the genome,  but the question is, \n",
    "\n",
    "> ### where is the hit in the genome?\n",
    "\n",
    "The only fact that it matched row 1 of the Burrows-Wheeler array (BWA) does me absolutely no good at all. \n",
    "\n",
    "Now we have BWT, OCC, and Count, we can compute the LF (Last-to-First, mapping last column to first column) function. How can we do with the information to locate the hit in the genome?\n",
    "\n",
    "#### 3.2.1 Naive solution 1\n",
    "\n",
    "We can use the \"**walk-left**\" to walk back to the beginning of the genome (that is, the row ending with `$` sign) and the number of steps to take is exactly the **offset of hit** in the genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "i = top\n",
    "j = 0\n",
    "while not bwt[i]=='$':\n",
    "    ch = bwt[i]\n",
    "    i = LF(ch, i)\n",
    "    j += 1\n",
    "print j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 2</font>\n",
    "\n",
    "If we have multiple hits, that is, there are more than one row betwen `top` and `bottom`, say, 2 and 5. What should we do to obtain all the hits (offsets in the genome)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xrange(1, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange(top, bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm, is also linear, but in the length of the genome, $\\mathcal{O}(n)$. Due to the large size of the genome, this could be too slow.\n",
    "\n",
    "But any other ideas can help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Naive solution 2: Rows to Reference Positions\n",
    "\n",
    "We can keep the whole suffix array in the memory. Thus it is a **lookup** in the array to find the reference position. That is, when we build the index, we need to store the offset for each row of the suffix array:\n",
    "> ### However, for human genome, the suffix array is $\\sim 12 GB$, which is too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Hybrid solution\n",
    "\n",
    "\n",
    "![](../images/bwt5.gif)\n",
    "Store not all, but sample of suffix array, and then \"walk left\" to the next sampled (a.k.a \"marked\") row to the left. This approach is proposed by Ferragina and Manzini in 2008. This, can cut down the storage demand.\n",
    "\n",
    "混合方法是上述两种策略的折中，只保存部分后缀数组，然后采用上述第一种“向左游走”策略，直到找到下一个“被标记”的后缀数组行为止。将“向左游走”的步数加上该行的“偏移量”，得到的结果就是输入检索序列在基因组上的偏移位置。\n",
    "\n",
    "Thus the final hit offset is the sum of the \"walk-left\" steps and the \"marked\" row offset of the sampled suffix array. \n",
    "\n",
    "One of the ngs short read aligner, Bowtie, \"marks\" every 32nd  row by default (but note that this can be modifiable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Improving the LF() function\n",
    "\n",
    "Now the alignment of short reads against the genome is linear. However, in the inner iteration, the `LF()` routine is still very time-consuming. `LF(ch, idx)`tries to determine the **rank** of *ch* in row $idx$, which means that we need to count the occurrences of `ch` in all previous rows. This is very expensive, although the rank is a very simple metric.\n",
    "\n",
    "![](../images/bwt6.gif)\n",
    "\n",
    "Instead of doing that, we can build a data structure that pre-calculate the cumulative counts for A/C/T/G up to **periodic checkpoints** in BWT. So when you need to compute the counts at an arbitrary point, you can go to the nearest checkpoint and make an adjustment by counting the number of characters betwen the index and the checkpoint. A very straight-forward approach, all right.\n",
    "\n",
    "![](../images/bwt7.gif)\n",
    "\n",
    "So now we know that the full index consists of \n",
    "- the **Burrows-Wheeler transform (BWT)** with the same size as $T$, \n",
    "- the **sampling of the suffix array** with around 50\\% of $T$\n",
    "- the **checkpoint** for the `LF()` function to make a constant time ($~15%$ of $T$)\n",
    "\n",
    "That's what is inside of an FM-Index. Therefore the size of full FM-index is about $1.65x$ the size of $T$.\n",
    "\n",
    "In Bowtie, \n",
    "- Two bits for the encoding of each base and no compression\n",
    "- 16 bytes checkpoint for every 448 characters\n",
    "- Use default suffix sampling rate\n",
    "\n",
    "We can see that it is very small, compared to things like **suffix trees ($>45x$)**, **suffix arrays ($>15x$) **, or even **other kinds of hash indexes ($>15x$) for looking for seeds**.\n",
    "\n",
    "So it's a very compact index, and thus it is very efficient. It is a very wonderful data structure, except that we have not dealt with mismatches yet, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### Oligomer counting\n",
    "- Healy J *et al*: Annotating large genomes with exact word matches. *Genome Res* 2003, 13(10):2306-2315.\n",
    "   \n",
    "#### Whole-genome alignment\n",
    "- Li H *et al*: Fast and accurate short read alignment with Burrows-Wheeler transform. *Bioinformatics* 2009, 25(14): 1754-1760 (bwa aligner)\n",
    "    \n",
    "#### Smith-Waterman alignment to large reference\n",
    "- Lam TW *et al*: Compressed indexing and local alignment of DNA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 3</font>\n",
    "\n",
    "Please organize the above python scripts into a object-oriented fashion.\n",
    "\n",
    "```python\n",
    "class BWT():\n",
    "    \"\"\"add your code here\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "And also run some examples to verify your codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: How to deal with mismatches?\n",
    "\n",
    "Once we are trapped by `top==bot`, which means that the query does not exactly occur in the genome. But we can change the current character until it can go ahead. This is called the **backtracking**.\n",
    "\n",
    "This method can also be applied when we want to take into accout the sequencing quality of the reads. We know that for a sequenced read, it is often of the poor quality in the 3' end. But the above alignment are starting from the right-most bases, so this often results in false hits. So for the bases with poor quality, we need to change the state since we are not confident of the sequenced results.\n",
    "\n",
    "Additionaly, to deal with the 3'-end bias of sequencing error, we can use the \"mirror\" index of the genome and also reverse the query string in order to get a better result.\n",
    "\n",
    "All these wonderful technique, can overcome some of the limitations of backtracking. Unfortunately, this will of course increase the computational burden. This requires you to make a balance between these two metrics - accuracy and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When creating BWT, Heng Li's **bwa** does not sort the entire Burrow-Wheeler array. Instead, he use an insertion-based method to achieve a better efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. The Alignment Pipeline\n",
    "\n",
    "The typical pipeline for alignment contains the following 5 steps:\n",
    "\n",
    "### 4.1 Obtain the reads and do some quality checking (QC)\n",
    "\n",
    "This is the data propressing step. In this step, besides obtaining the reads data, you need to trim the adapters and the poor-quality bases/reads from the data.\n",
    "\n",
    "The typical tools for this step usually include:\n",
    "- **[SRA Toolkit](https://github.com/ncbi/sra-tools)**'s `fastq-dump` for extracting `*.fastq` out of the `*.sra` file;\n",
    "- **[FastQC](www.bioinformatics.babraham.ac.uk/projects/fastqc)** for quality checking of the `fastq` files;\n",
    "- **[Picard Tools](https://broadinstitute.github.io/picard)** for marking the duplicates;\n",
    "- **[Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)** for low-quality trimming of the reads;\n",
    "- **[FastX Toolkit](http://hannonlab.cshl.edu/fastx_toolkit)** for processing the reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Conduct the alignment (mapping)\n",
    "\n",
    "To accomplish the task, you need to finish the following jobs:\n",
    "- Index the genome sequence;\n",
    "- Align the reads to the genome;\n",
    "- Sort the alignments;\n",
    "- Merge the alignments;\n",
    "- Recalibrate the alignments;\n",
    "- Merge the libraries;\n",
    "- Index the final alignments. \n",
    "\n",
    "The typical tools for this step include:\n",
    "- The **alignment tools** (e.g. [bwa](http://bio-bwa.sourceforge.net/); [bowtie](bowtie-bio.sourceforge.net/); [SOAP](http://soap.genomics.org.cn/)) \n",
    "- **[samtools](http://samtools.sourceforge.net/)**: providing various utilities for manipulating alignments in the [SAM](http://samtools.sourceforge.net/SAM1.pdf) format, including sorting, merging, indexing and generating alignments in a per-position format. \n",
    "- **[GATK](https://software.broadinstitute.org/gatk/)**: offering a wide variety of tools with a primary focus on variant discovery and genotyping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 `bwa`\n",
    "\n",
    "- `bwa index  [-a bwtsw|div|is] [-c] <in.fasta>`\n",
    "    * **Burrows-Wheeler Transform** construction algorithm with \"bwtsw\" for vertebrate-size genomes and \"is\" for smaller genomes.\n",
    "    \n",
    "- `bwa aln [options] <prefix> in.fq`\n",
    "    * aligns each single-ended fastq file individually;\n",
    "    * `<prefix>` is the name of the reference file spcified by `bwa index`;\n",
    "    * `[options]` controls alignment parameters, scoring matrix, seed length, etc.\n",
    "\n",
    "- `bwa sampe <prefix> <in1.sai> <in2.sai> <in1.fq> <in2.fq>`\n",
    "    * generates pairwise alignment from both `<in1.sai>` and `<in2.sai>` files produced by `bwa aln`;\n",
    "    * for paired-ended reads stored in `<in1.fq>` and `<in2.fq>`;\n",
    "    * produces `SAM` outputs.\n",
    "\n",
    "- `bwa bwasw <prefix> <query.fa>`\n",
    "    * aligns long reads in `<query.fa>` to the reference named `<prefix>`;\n",
    "    * produces `SAM`-formatted outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">`bwa` usage notes</font>\n",
    "- `bwa` finds matches up to a preset **edit distance** (by default, for a 100 bases allowing 5 edits)\n",
    "- `bwa` use `-q` to set clipping for **quality threshold**, e.g. 20.\n",
    "- **Non-ACGT** bases are treated as **mismatch**.\n",
    "- **Parallelization** for speed:\n",
    "    * split data into 1Gb blocks;\n",
    "    * each 1Gb blocks takes around 8 hours.\n",
    "- Check for truncated BAM files (e.g. using `samtools flagstats`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 How to further improve the alignments?\n",
    "\n",
    "To improve the alignments generated by `bwa`, you need to do the following steps:\n",
    "- **Remove the duplicates in the library**, which can utilize such tools like **samtools** and **Picard**.\n",
    "- **Realignment around the indels** and **Base quality calibration** using **GATK**. \n",
    "\n",
    "\n",
    "### Library duplicate removal\n",
    "\n",
    "PCR amplification is essential in the sequencing of **low-input** DNA samples. But unfortunately, this will result in duplicate DNA fragments, which can further lead to false SNP calls.\n",
    "\n",
    "To remove the duplicates, you can first **identify the read pairs whose outer ends map to the same position** on the genome and then **remove all but one copy**. This can be done by the two similar commands:\n",
    "- `samtools rmdup`\n",
    "- `Picard/GATK MarkDuplicates`\n",
    "\n",
    "### Realignment\n",
    "\n",
    "Short indels in the sample relative to the reference can pose difficulties for alignment task since\n",
    "- indels near the ends of the reads are often not aligned correctly;\n",
    "- the aligners prefer naturally to introducing SNPs rather than indels.\n",
    "\n",
    "But how to do realignment?  Usually, the realignment algorithm works by\n",
    "- Input set of known indels (e.g., previously publised indel sites, dbSNPs, 1K-Genomes, or estimated directly from the alignments) and a BAM file;\n",
    "- At each site, model the indels and the reference haplotype, and select best fit with data;\n",
    "- Generate and modify new BAM files where the indels have been introduced through the realignment;\n",
    "- This can be implemented in GATK (indelRealign task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Additional alignment issues\n",
    "\n",
    "We'd better separate chromosome BAM files in order to parallelly process them easier.\n",
    "\n",
    "Moreover, for unmapped reads, we'd better conduct a further realignment to avoid missing due to reference incompatibillity or incompleteness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BAM/SAM\n",
    "\n",
    "SAM (Sequence Alignment/Map format) is an unified format for storing NGS alignment against the reference genome.\n",
    "\n",
    "Here are the some features for SAM/BAM format:\n",
    "- SAM file stores the alignment information from most alignment tools;\n",
    "- SAM supports almost all the sequencing technologies;\n",
    "- SAM supports indexing for quick retrieval;\n",
    "- Reads in the SAM file can be classified into logical groups (e.g., lanes, libraries, or individuals)\n",
    "\n",
    "The BAM (binary alignment/map format) is the binary equivalence of SAM.\n",
    "\n",
    "### 5.1 SAM format\n",
    "\n",
    "A typical SAM file contains two sections - the HEADER section and the ALIGNMENT section.\n",
    "\n",
    "The ALIGNMENT section stores the alignment information for the reads with each one line. Each line contains 11 mandatory fields and several optional fields (formatted as TAG TYPE VALUE). Here is the table for the list of mandatory fields:\n",
    "\n",
    "| Col | Field | Type | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 |  QNAME | str | query name of the read or the read pair |\n",
    "| 2 | FLAG | int | bitwise flags (pairing, mapped, mate mapped, etc.) |\n",
    "| 3 | RNAME | str | reference sequence name |\n",
    "| 4 | POS | int | 1-based leftmost position of clipped alignment |\n",
    "| 5 | MAPQ | int | PHRED-scale mapping quality |\n",
    "| 6 | CIGAR | str | extended CIGAR string for alignment details |\n",
    "| 7 | RNEXT | str | mate reference name ('=' for the same) |\n",
    "| 8 | PNEXT | int | position of mate/next segment |\n",
    "| 9 | TLEN | int | observed template length |\n",
    "| 10 | SEQ | str | segment sequence |\n",
    "| 11 | QUAL | str | ASCII of PHRED-scale base quality | \n",
    "\n",
    "For further about SAM/BAM format, please refer to https://samtools.github.io/hts-specs/SAMv1.pdf. \n",
    "\n",
    "For FLAG information, you can refer to http://picard.sourceforge.net/explain-flags.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 4</font>\n",
    "\n",
    "Try to interpret the following one-line SAM record:\n",
    "```\n",
    "IL4_315:7:105:408:43    177    X    1741    0    1S35M    X    56845228    0    ATTTGGCTCTCTGCTTGTTTATTATTGGTGTATNGG      +1,1+16;>;166>;>;;>>;>>>>>>,>>>>>+>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SAM/BAM file processing tools\n",
    "- **`samtools`**: C program and library\n",
    "    * http://samtools.sourceforge.net\n",
    "    * `samtools view`: SAM-BAM conversion\n",
    "    * `samtools sort`: sort the SAM records according to the positions\n",
    "    * `samtools index`:  create the index for SAM records\n",
    "    * `samtools merge`: merge  multiple BAM files\n",
    "    * `samtools flagstats`: summarizes the mapping flags\n",
    "    \n",
    "- **`Picard`**: Java program suite\n",
    "    * http://picard.sourceforge.net\n",
    "    * `MarkDuplicates`, `CollectAlignmentSummaryMetrics`, `CreateSequeceDictionary`, `SamToFastq`, `MeanQualityByCycle`\n",
    "\n",
    "- **`Pysam`**: Python interface to samtools\n",
    "    * http://code.google.com/p/pysam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variant Calling\n",
    "\n",
    "The vaiant calling step will generate a great deal of calling\n",
    "- Single nucleotide variants (SNVs) with genotype information (homozygous or heterozygous)\n",
    "- indels and\n",
    "- structural variants (SVs)\n",
    "\n",
    "This is a list of variant calling tools:\n",
    "- [samtools](http://samtools.sourceforge.net), [bcftools](http://bcftools.sourceforge.net)\n",
    "- [GATK](http://gatk.broadinstitute.org), [SOAPsnp](http://soap.genomics.org.cn/soapsnp.html), [Dindel](https://sites.google.com/site/keesalbers/soft/dindel)\n",
    "- [SVMerge](http://svmerge.sourceforge.net)\n",
    "\n",
    "These calling tools will generate a resulting file in [VCF](https://samtools.github.io/hts-specs/VCFv4.2.pdf) or [pileup](samtools.sourceforge.net/pileup.shtml).\n",
    "\n",
    "The calling and the filtering protocols will take advantages of the depth, quality, strand bias as well as the multiple sample information to accomplish the calling task.\n",
    "\n",
    "Compared to SNPs, the indels and the structural variants are much harder to call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Variant call format (VCF)\n",
    "\n",
    "A VCF file stores the polymorphism information (SNVs, indels, and SVs) with annotations across multiple samples. Besisdes, a VCF file also contains the metadata information such as the **dbSNP accession, filter status, and validation status**.\n",
    "\n",
    "Moreover, arbitrary tags can be added to the VCF file to be used to describe new types of variants. \n",
    "\n",
    "Similar to SAM, VCF can also be indexed for fast retrieval.\n",
    "\n",
    "Finally, a VCF can be generated from piping between samtools and bcftools:\n",
    "```bash\n",
    "samtools mpileup | bcftools view\n",
    "```\n",
    "\n",
    "BCF is the binary form of VCF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$Exercise 5: Variant Call Format (VCFs)</font>\n",
    "\n",
    "Try your best to interpret the follwowing \"*.vcf\" files:\n",
    "```\n",
    "#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    sample1    sample2    sample3\n",
    "3    74393    .    G    T     999    .    DP=31;AF1=0.7002;AC1=4;DP4=4,0,22,2    ;... GT:PL:DP:GQ    1/1:181, 57, 0:19:57    1/1:90,15,0:5:16    0/0:0,12,85:4:7\n",
    "```\n",
    "\n",
    "See H, Li. Bioinformatics, 27(21): 2987-2993 (2011) for details of likelihood and population genetics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Structural variants\n",
    "\n",
    "The structural variants (SVs) includes the following types:\n",
    "- large insertions/deletions\n",
    "- inversion\n",
    "- translocation\n",
    "\n",
    "![insert some diagram here]()\n",
    "\n",
    "Read and pairing information can be used to detect these events:\n",
    "- Unexpected fragment size\n",
    "- presense/absence of mate pairs\n",
    "- excessive/reduced read depths (CNVs, copy number variants)\n",
    "\n",
    "[SVMerge]() pipeline can be used to call the structural variants:\n",
    "- makes SVs predictions using a collection of callers;\n",
    "- input is one BAM per sample;\n",
    "- callers run individually and the outputs were converted into standard BED format;\n",
    "- the calls were merged;\n",
    "- the results are computationally validated using local de-novo assembly.\n",
    "- http://svmerge.sourceforge.net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Short Reads Assembly\n",
    "\n",
    "Here are the tools for doing short reads assembly:\n",
    "- [Abyss](http://www.bcgsc.ca/platform/bioinfo/software/abyss): De-novo, parallel, paired-end short-read assembler.\n",
    "- [SGA](https://github.com/jts/sga): Efficient de-novo large genome assembler based on the concepts of string graphs.\n",
    "- [SOAPdenovo](http://soap.genome.org.cn/soapdenovo.html): de Bruijn graph based genome assembler.\n",
    "- [ALLPATHS-LG](http://www.broadinstitute.org/software/allpaths-lg/blog): \n",
    "- [Cortex](http://cortexassembler.sourceforge.net)\n",
    "- [Velvet](http://www.ebi.ac.uk/~zerbino/velvet)\n",
    "\n",
    "Recent assembly evaluation projects such as [Assemblathon]() and [GAGE]() can be used to assess the efficiency and power of these assemblers.  And the related metrics include assembly coverage and lengths.\n",
    "\n",
    "### 7.1 Assembly metrics\n",
    "\n",
    "- N10, N50, N90\n",
    "    * $x\\%$ of assembly is in fragments larger than $Nx$;\n",
    "    \n",
    "- Number of contigs\n",
    "\n",
    "- Mean/max contig lengths\n",
    "\n",
    "- Realignment\n",
    "    * Fraction of read pairs mapped correctly;\n",
    "    * Correct homozygous SNPs\n",
    "    * Identify breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
